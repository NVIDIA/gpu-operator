apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nvidia-device-plugin-daemonset
  name: nvidia-device-plugin-daemonset
  namespace: "FILLED BY THE OPERATOR"
  annotations:
    openshift.io/scc: hostmount-anyuid
spec:
  selector:
    matchLabels:
      app: nvidia-device-plugin-daemonset
  template:
    metadata:
      labels:
        app: nvidia-device-plugin-daemonset
    spec:
      nodeSelector:
        nvidia.com/gpu.deploy.device-plugin: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
      priorityClassName: system-node-critical
      serviceAccountName: nvidia-device-plugin
      initContainers:
      - image: "FILLED BY THE OPERATOR"
        name: toolkit-validation
        command: ['sh', '-c']
        args: ["until [ -f /run/nvidia/validations/toolkit-ready ]; do echo waiting for nvidia container stack to be setup; sleep 5; done"]
        securityContext:
          privileged: true
        volumeMounts:
          - name: run-nvidia-validations
            mountPath: /run/nvidia/validations
            mountPropagation: HostToContainer
      # The cdi-validation init container will only be added when CDI is enabled in the operator.
      # This init container ensures that the device-plugin does not start until a version of
      # the toolkit container that has CDI enabled is running. This is implemented to prevent
      # a possible race condition where a newer version of the device-plugin, with CDI enabled,
      # starts running while an older version of the toolkit, which does not have CDI enabled,
      # is still running.
      - image: "FILLED BY THE OPERATOR"
        name: cdi-validation
        command: [ 'sh', '-c' ]
        args:
          - |
            if [ -z "${NVIDIA_CTK_LIBCUDA_DIR}" ]; then
              echo "waiting for NVIDIA Container Toolkit to be installed with CDI enabled"
              echo "will sleep and then exit"
              sleep 5
              exit 1
            fi
        env:
          - name: NVIDIA_VISIBLE_DEVICES
            value: all
        securityContext:
          privileged: true
        volumeMounts:
          - name: run-nvidia-validations
            mountPath: /run/nvidia/validations
            mountPropagation: HostToContainer
      - image: "FILLED BY THE OPERATOR"
        name: config-manager-init
        command: ["config-manager"]
        env:
        - name: ONESHOT
          value: "true"
        - name: KUBECONFIG
          value: ""
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: "spec.nodeName"
        - name: NODE_LABEL
          value: "nvidia.com/device-plugin.config"
        - name: CONFIG_FILE_SRCDIR
          value: "/available-configs"
        - name: CONFIG_FILE_DST
          value: "/config/config.yaml"
        - name: DEFAULT_CONFIG
          value: ""
        - name: SEND_SIGNAL
          value: "false"
        - name: SIGNAL
          value: ""
        - name: PROCESS_TO_SIGNAL
          value: ""
      containers:
      - image: "FILLED BY THE OPERATOR"
        name: nvidia-device-plugin
        command: ["/bin/sh", "-c"]
        args:
          - /bin/entrypoint.sh
        securityContext:
          privileged: true
        env:
          - name: PASS_DEVICE_SPECS
            value: "true"
          - name: FAIL_ON_INIT_ERROR
            value: "true"
          - name: DEVICE_LIST_STRATEGY
            value: envvar
          - name: DEVICE_ID_STRATEGY
            value: uuid
          - name: NVIDIA_VISIBLE_DEVICES
            value: all
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: all
          - name: MPS_ROOT
            value: /run/nvidia/mps
        volumeMounts:
          - name: nvidia-device-plugin-entrypoint
            readOnly: true
            mountPath: /bin/entrypoint.sh
            subPath: entrypoint.sh
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
          - name: run-nvidia-validations
            mountPath: /run/nvidia/validations
          - name: driver-install-dir
            mountPath: /driver-root
            mountPropagation: HostToContainer
          - name: host-root
            mountPath: /host
            readOnly: true
            mountPropagation: HostToContainer
          - name: cdi-root
            mountPath: /var/run/cdi
          # The MPS /dev/shm is needed to allow for MPS daemon health-checking.
          - name: mps-shm
            mountPath: /dev/shm
          - name: mps-root
            mountPath: /mps
      - image: "FILLED BY THE OPERATOR"
        name: config-manager
        command: ["config-manager"]
        securityContext:
          privileged: true
        env:
        - name: ONESHOT
          value: "false"
        - name: KUBECONFIG
          value: ""
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: "spec.nodeName"
        - name: NODE_LABEL
          value: "nvidia.com/device-plugin.config"
        - name: CONFIG_FILE_SRCDIR
          value: "/available-configs"
        - name: CONFIG_FILE_DST
          value: "/config/config.yaml"
        - name: DEFAULT_CONFIG
          value: ""
        - name: SEND_SIGNAL
          value: "true"
        - name: SIGNAL
          value: "1" # SIGHUP
        - name: PROCESS_TO_SIGNAL
          value: "nvidia-device-plugin"
      volumes:
        - name: nvidia-device-plugin-entrypoint
          configMap:
            name: nvidia-device-plugin-entrypoint
            defaultMode: 448
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins
        - name: run-nvidia-validations
          hostPath:
            path: "/run/nvidia/validations"
            type: DirectoryOrCreate
        - name: driver-install-dir
          hostPath:
            path: "/run/nvidia/driver"
            type: DirectoryOrCreate
        - name: host-root
          hostPath:
            path: /
        - name: cdi-root
          hostPath:
            path: /var/run/cdi
            type: DirectoryOrCreate
        - name: mps-root
          hostPath:
            path: /run/nvidia/mps
            type: DirectoryOrCreate
        - name: mps-shm
          hostPath:
            path: /run/nvidia/mps/shm
