include:
  - local: '.common-ci.yml'
  - project: nvidia/container-infrastructure/aws-kube-ci
    file: aws-kube-ci.yml
    ref: 23.09.29

variables:
  GIT_SUBMODULE_PATHS: aws-kube-ci

helm-lint:
  stage: config-checks
  allow_failure: true
  image:
    name: alpine/git
    entrypoint: ['/usr/bin/env']
  before_script:
    - apk add --update curl curl-dev openssl bash git openssh make
    - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
  script:
    - helm lint deployments/gpu-operator/

.dev-image:
  variables:
    IMAGE_NAME: "${CI_REGISTRY_IMAGE}"
    IMAGE_TAG: "${CI_COMMIT_REF_SLUG}"

build-dev-image:
  extends:
    - .dev-image
  stage: image
  script:
    - apk --no-cache add make bash
    - make .build-image
    - docker login -u "${CI_REGISTRY_USER}" -p "${CI_REGISTRY_PASSWORD}" "${CI_REGISTRY}"
    - make .push-build-image

.requires-build-image:
  image: ${CI_REGISTRY_IMAGE}:${CI_COMMIT_REF_SLUG}-build

.config-check:
  extends:
    - .requires-build-image
  stage: config-checks

validate-csv:
  extends:
    - .config-check
  script:
    - make validate-csv

validate-helm-values:
  extends:
    - .config-check
  before_script:
    - curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
  script:
    - make validate-helm-values

go-checks:
  extends:
    - .requires-build-image
  stage: go-checks
  script:
    - make check

go-build:
  extends:
    - .requires-build-image
  stage: go-build
  script:
    - make build

unit-tests:
  extends:
    - .requires-build-image
  stage: unit-tests
  script:
    - make coverage

# Define the image build targets
.image-build:
  stage: image-build
  variables:
    IMAGE_NAME: "${CI_REGISTRY_IMAGE}"
    VERSION: "${CI_COMMIT_SHORT_SHA}"
    PUSH_ON_BUILD: "true"
  before_script:
    - !reference [.buildx-setup, before_script]
    - apk add --no-cache bash make
    - 'echo "Logging in to CI registry ${CI_REGISTRY}"'
    - docker login -u "${CI_REGISTRY_USER}" -p "${CI_REGISTRY_PASSWORD}" "${CI_REGISTRY}"
  script:
    - make build-${DIST}

build:gpu-operator:
  extends:
    - .image-build
    - .dist-ubi9
    - .target-gpu-operator

build:gpu-operator-validator:
  extends:
    - .image-build
    - .dist-ubi9
    - .target-gpu-operator-validator

.e2e_defaults:
  variables:
    TF_VAR_project_name: "gpu-operator"
    TF_VAR_kubernetes_version: "v1.27.6"
    TF_VAR_additional_ingress_ip_ranges: '["216.228.112.0/26", "217.111.27.192/26"]'
    # These should match the images generated by the deploy step.
    # TODO: Should these use the staging release instead?
    OPERATOR_VERSION: "${CI_COMMIT_SHORT_SHA}-${DIST}"
    OPERATOR_IMAGE: "${CI_REGISTRY_IMAGE}"
    VALIDATOR_VERSION: "${CI_COMMIT_SHORT_SHA}-${DIST}"
    VALIDATOR_IMAGE: "${CI_REGISTRY_IMAGE}/gpu-operator-validator"
    GPU_PRODUCT_NAME: "Tesla-T4"
  extends:
    - .dist-ubi9
  except:
    variables:
      - $CI_COMMIT_MESSAGE =~ /skip-end-to-end-tests/
      - $SKIP_TESTS =~ /^y/
      - $CI_PIPELINE_SOURCE == "schedule"

.e2e_tests:
  extends:
    - .e2e_defaults
  stage: e2e_tests
  image: alpine
  variables:
    SKIP_LAUNCH: "true"
  before_script:
    - apk add --no-cache openssh-client rsync bash
  script:
    - source aws-kube-ci/hostname
    - export private_key="${CI_PROJECT_DIR}/aws-kube-ci/${private_key}"
    - export instance_hostname="${instance_hostname}"
    - export TEST_CASE="${TEST_CASE}"
    - rc=0
    - ${CI_PROJECT_DIR}/tests/ci-run-e2e.sh ${OPERATOR_IMAGE} ${OPERATOR_VERSION} ${VALIDATOR_IMAGE} ${VALIDATOR_VERSION} ${GPU_PRODUCT_NAME} ${TEST_CASE} || rc=$?
    - ${CI_PROJECT_DIR}/tests/scripts/pull.sh /tmp/logs logs
    - exit $rc
  artifacts:
    when: always
    paths:
      - logs/

e2e_tests_containerd:
  extends:
    - .e2e_tests
  dependencies:
    - aws_kube_setup_containerd
  needs:
    - aws_kube_setup_containerd
  variables:
    CONTAINER_RUNTIME: "containerd"
    TEST_CASE: "./tests/cases/defaults.sh"

e2e_tests_nvidiadriver:
  extends:
    - .e2e_tests
  dependencies:
    - aws_kube_setup_nvidiadriver
  needs:
    - aws_kube_setup_nvidiadriver
  variables:
    CONTAINER_RUNTIME: "containerd"
    TEST_CASE: "./tests/cases/nvidia-driver.sh"

.launch_infra:
  extends:
    - .aws_kube_setup
    - .e2e_defaults
  variables:
    TF_VAR_project_name: "gpu-operator"
    TF_VAR_additional_ingress_ip_ranges: '["216.228.112.0/26", "217.111.27.192/26"]'

aws_kube_setup_containerd:
  extends:
    - .launch_infra
  variables:
    TF_VAR_container_runtime: "containerd"
    TF_VAR_project_name: "gpu-operator-containerd"

aws_kube_setup_nvidiadriver:
  extends:
    - .launch_infra
  variables:
    TF_VAR_container_runtime: "containerd"
    TF_VAR_project_name: "gpu-operator-nvidiadriver"

aws_kube_clean_containerd:
  extends:
    - .aws_kube_clean
    - .e2e_defaults
  variables:
    TF_VAR_container_runtime: "containerd"
    TF_VAR_project_name: "gpu-operator-containerd"
  dependencies:
    - aws_kube_setup_containerd

aws_kube_clean_nvidiadriver:
  extends:
    - .aws_kube_clean
    - .e2e_defaults
  variables:
    TF_VAR_container_runtime: "containerd"
    TF_VAR_project_name: "gpu-operator-nvidiadriver"
  dependencies:
    - aws_kube_setup_nvidiadriver
